<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · KLayers.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="assets/documenter.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link href="assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>KLayers.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="index.html">Home</a></li><li><span class="toctext">Manual</span><ul><li class="current"><a class="toctext" href="reference.html">Reference</a><ul class="internal"><li><a class="toctext" href="#Core-1">Core</a></li><li><a class="toctext" href="#Nonlinear-1">Nonlinear</a></li><li><a class="toctext" href="#MLP-1">MLP</a></li><li><a class="toctext" href="#RNN-1">RNN</a></li><li><a class="toctext" href="#Function-Index-1">Function Index</a></li></ul></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Manual</li><li><a href="reference.html">Reference</a></li></ul><a class="edit-page" href="https://github.com/ekinakyurek/KLayers/blob/master/docs/src/reference.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Reference</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Reference-1" href="#Reference-1">Reference</a></h1><p><strong>Contents</strong></p><ul><li><a href="reference.html#Reference-1">Reference</a></li><ul><li><a href="reference.html#Core-1">Core</a></li><li><a href="reference.html#Nonlinear-1">Nonlinear</a></li><li><a href="reference.html#MLP-1">MLP</a></li><li><a href="reference.html#RNN-1">RNN</a></li><li><a href="reference.html#Function-Index-1">Function Index</a></li></ul></ul><h2><a class="nav-anchor" id="Core-1" href="#Core-1">Core</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Embed" href="#KLayers.Embed"><code>KLayers.Embed</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Embed(inputSize,embedSize;winit=xavier)</code></pre><p>Creates and embedding layer according to given <code>inputSize</code> and <code>embedSize</code>.</p><p>By default embedding parameters initialized with xavier, you can change it <code>winit</code> argument</p><pre><code class="language-none">(m::Embed)(x::Array{T}) where T&lt;:Integer
(m::Embed)(x)</code></pre><p>Embed objects are callable with an input which is either and integer array (one hot encoding) or an N-dimensional matrix. For N-dimensional matrix, <code>size(x,1)==inputSize</code></p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/core.jl#L1-L19">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Linear" href="#KLayers.Linear"><code>KLayers.Linear</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Linear(inputSize,outputSize;kwargs...)
(m::Linear)(x) #forward run</code></pre><p>Creates and linear layer according to given <code>inputSize</code> and <code>outputSize</code>.</p><p>By default embedding parameters initialized with xavier, you can change it <code>winit</code> argument</p><p><strong>Keywords</strong></p><ul><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/core.jl#L34-L49">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Dense" href="#KLayers.Dense"><code>KLayers.Dense</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Dense(inputSize,outputSize;kwargs...)
(m::Dense)(x) #forward run</code></pre><p>Creates and deense layer according to given <code>inputSize</code> and <code>outputSize</code>.</p><p><strong>Keywords</strong></p><ul><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>f=ReLU()</code>: activation function</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/core.jl#L57-L69">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Conv" href="#KLayers.Conv"><code>KLayers.Conv</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Conv(h,[w,c,o];kwargs...)
(m::Conv)(x) #forward run</code></pre><p>Creates and convolutional layer according to given filter dimensions.</p><p><strong>Keywords</strong></p><ul><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>padding=0</code>: the number of extra zeros implicitly concatenated at the start and at the end of each dimension.</li><li><code>stride=1</code>: the number of elements to slide to reach the next filtering window.</li><li><code>upscale=1</code>: upscale factor for each dimension.</li><li><code>mode=0</code>: 0 for convolution and 1 for cross-correlation.</li><li><code>alpha=1</code>: can be used to scale the result.</li><li><code>handle</code>: handle to a previously created cuDNN context. Defaults to a Knet allocated handle.</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/core.jl#L78-L95">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.BatchNorm" href="#KLayers.BatchNorm"><code>KLayers.BatchNorm</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">BatchNorm(channels:Int;o...)
(m::BatchNorm)(x;o...) #forward run</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/core.jl#L127-L130">source</a></section><h2><a class="nav-anchor" id="Nonlinear-1" href="#Nonlinear-1">Nonlinear</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.ReLU" href="#KLayers.ReLU"><code>KLayers.ReLU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ReLU()
(l::ReLU)(x) = max(0,x)</code></pre><p>Fast kernel is avaiable for gpu</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L1-L6">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Sigm" href="#KLayers.Sigm"><code>KLayers.Sigm</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Sigm()
(l::Sigm)(x) = sigm(x)</code></pre><p>Sigmoid function</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L11-L16">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Tanh" href="#KLayers.Tanh"><code>KLayers.Tanh</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Tanh()
(l::Tanh)(x) = tanh(x)</code></pre><p>Tangent hyperbolic function</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L21-L26">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.ELU" href="#KLayers.ELU"><code>KLayers.ELU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">ELU()
(l::ELU)(x) = elu(x) -&gt; Computes x &lt; 0 ? exp(x) - 1 : x</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L32-L35">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.LeakyReLU" href="#KLayers.LeakyReLU"><code>KLayers.LeakyReLU</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LeakyReLU(α=0.2)
(l::ELU)(x) -&gt; Computes x &lt; 0 ? α*x : x</code></pre></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L40-L43">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.Dropout" href="#KLayers.Dropout"><code>KLayers.Dropout</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">Dropout(p=0)</code></pre><p>Dropout Layer. <code>p</code> is the droput probability.</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L50-L54">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.SoftMax" href="#KLayers.SoftMax"><code>KLayers.SoftMax</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SoftMax(dims=:)
(l::SoftMax)(x)</code></pre><p>Treat entries in x as as unnormalized scores and return softmax probabilities.</p><p>dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions. In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L76-L84">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.LogP" href="#KLayers.LogP"><code>KLayers.LogP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LogP(dims=:)
(l::LogP)(x)</code></pre><p>Treat entries in x as as unnormalized log probabilities and return normalized log probabilities.</p><p>dims is an optional argument, if not specified the normalization is over the whole x, otherwise the normalization is performed over the given dimensions. In particular, if x is a matrix, dims=1 normalizes columns of x and dims=2 normalizes rows of x.</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L61-L69">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.LogSumExp" href="#KLayers.LogSumExp"><code>KLayers.LogSumExp</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">LogSumExp(dims=:)
(l::LogSumExp)(x)</code></pre><p>Compute log(sum(exp(x);dims)) in a numerically stable manner.</p><p>dims is an optional argument, if not specified the summation is over the whole x, otherwise the summation is performed over the given dimensions. In particular if x   is a matrix, dims=1 sums columns of x and dims=2 sums rows of x.</p></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/nonlinear.jl#L91-L99">source</a></section><h2><a class="nav-anchor" id="MLP-1" href="#MLP-1">MLP</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.MLP" href="#KLayers.MLP"><code>KLayers.MLP</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">MLP(h::Int...;kwargs...)</code></pre><p>Creates a multi layer perceptron according to given hidden states. First hidden state is equal to input size and the last one equal to output size.</p><pre><code class="language-none">(m::MLP)(x;prob=0)</code></pre><p>Runs MLP with given input <code>x</code>. <code>prob</code> is the dropout probability.</p><p><strong>Keywords</strong></p><ul><li><code>winit=xavier</code>: weight initialization distribution</li><li><code>bias=zeros</code>: bias initialization distribution</li><li><code>f=ReLU()</code>: activation function</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/mlp.jl#L1-L19">source</a></section><h2><a class="nav-anchor" id="RNN-1" href="#RNN-1">RNN</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="KLayers.RNN" href="#KLayers.RNN"><code>KLayers.RNN</code></a> — <span class="docstring-category">Type</span>.</div><div><div><pre><code class="language-none">SRNN(inputSize,hiddenSize;activation=:relu,options...)
LSTM(inputSize,hiddenSize;options...)
GRU(inputSize,hiddenSize;options...)

(1) (l::T)(x;kwargs...) where T&lt;:RNN
(2) (l::T)(x::Array{Int};batchSizes=nothing,kwargs...) where T&lt;:RNN
(3) (l::T)(x::Vector{Vector{Int}};sorted=false,kwargs...) where T&lt;:RNN</code></pre><p>All RNN layers has above forward run(1,2,3) functionalities.</p><p>(1) <code>x</code> is an input array with size equals d,[B,T]</p><p>(2) For this You need to have an <code>RNN</code> with embedding layer. <code>x</code> is an integer array and inputs coressponds one hot vector indices. You can give 2D array for minibatching as rows corresponds to one instance. You can give 1D array with minibatching by specifying batch batchSizes argument. Checkout <code>Knet.rnnforw</code> for this.</p><p>(3) For this You need to have an <code>RNN</code> with embedding layer. <code>x</code> is an vector of integer vectors. Every integer vector corresponds to an instance. It automatically batches inputs. It is better to give inputs as sorted. If your inputs sorted you can make <code>sorted</code> argument true to increase performance.</p><p>Outputs of the forward functions are always <code>y,h,c,indices</code>. <code>h</code>,<code>c</code> and <code>indices</code> may be nothing depending on the kwargs you used in forward.</p><p><code>y</code> is last hidden states of each layer. <code>size(y)=(H/2H,[B,T])</code>. If you use batchSizes argument <code>y</code> becomes 2D array <code>size(y)=(H/2H,sum(batchSizes))</code>. To get correct hidden states for an instance in your batch you should use indices output.</p><p><code>h</code> is the hidden states in each timesstep. <code>size(h) = h,B,L/2L</code></p><p><code>c</code> is the hidden states in each timesstep. <code>size(h) = h,B,L/2L</code></p><p><code>indices</code> is corresponding indices for your batches in <code>y</code> if you used batchSizes. To get ith instance&#39;s hidden states in each times step, you may type: <code>y[:,indices[i]]</code> `</p><p><strong>options</strong></p><ul><li><code>embed=nothing</code>: embedding size or and embedding layer</li><li><code>numLayers=1</code>: Number of RNN layers.</li><li><code>bidirectional=false</code>: Create a bidirectional RNN if <code>true</code>.</li><li><code>dropout=0</code>: Dropout probability. Ignored if <code>numLayers==1</code>.</li><li><code>skipInput=false</code>: Do not multiply the input with a matrix if <code>true</code>.</li><li><code>dataType=Float32</code>: Data type to use for weights.</li><li><code>algo=0</code>: Algorithm to use, see CUDNN docs for details.</li><li><code>seed=0</code>: Random number seed for dropout. Uses <code>time()</code> if 0.</li><li><code>winit=xavier</code>: Weight initialization method for matrices.</li><li><code>binit=zeros</code>: Weight initialization method for bias vectors.</li><li><code>usegpu=(gpu()&gt;=0)</code>: GPU used by default if one exists.</li></ul><p><strong>kwargs</strong></p><ul><li>hx=nothing : initial hidden states</li><li>cx=nothing : initial memory cells</li><li>hy=false   : if true returns h</li><li>cy=false   : if true returns c</li></ul></div></div><a class="source-link" target="_blank" href="https://github.com/ekinakyurek/KLayers/blob/7e8846a47662a536018f453c352d33e496863d45/src/rnn.jl#L1-L63">source</a></section><h2><a class="nav-anchor" id="Function-Index-1" href="#Function-Index-1">Function Index</a></h2><ul><li><a href="reference.html#KLayers.BatchNorm"><code>KLayers.BatchNorm</code></a></li><li><a href="reference.html#KLayers.Conv"><code>KLayers.Conv</code></a></li><li><a href="reference.html#KLayers.Dense"><code>KLayers.Dense</code></a></li><li><a href="reference.html#KLayers.Dropout"><code>KLayers.Dropout</code></a></li><li><a href="reference.html#KLayers.ELU"><code>KLayers.ELU</code></a></li><li><a href="reference.html#KLayers.Embed"><code>KLayers.Embed</code></a></li><li><a href="reference.html#KLayers.LeakyReLU"><code>KLayers.LeakyReLU</code></a></li><li><a href="reference.html#KLayers.Linear"><code>KLayers.Linear</code></a></li><li><a href="reference.html#KLayers.LogP"><code>KLayers.LogP</code></a></li><li><a href="reference.html#KLayers.LogSumExp"><code>KLayers.LogSumExp</code></a></li><li><a href="reference.html#KLayers.MLP"><code>KLayers.MLP</code></a></li><li><a href="reference.html#KLayers.RNN"><code>KLayers.RNN</code></a></li><li><a href="reference.html#KLayers.ReLU"><code>KLayers.ReLU</code></a></li><li><a href="reference.html#KLayers.Sigm"><code>KLayers.Sigm</code></a></li><li><a href="reference.html#KLayers.SoftMax"><code>KLayers.SoftMax</code></a></li><li><a href="reference.html#KLayers.Tanh"><code>KLayers.Tanh</code></a></li></ul><footer><hr/><a class="previous" href="index.html"><span class="direction">Previous</span><span class="title">Home</span></a></footer></article></body></html>
